{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adacdb86-ea83-4729-8a33-71701e414333",
   "metadata": {},
   "source": [
    "## PySpark Cheat Sheet\n",
    "\n",
    "Apache Spark and Apache Hadoop are both open-source frameworks for big data processing. Spark can be 100x faster than Hadoop for large scale data processing, however, Hadoop has distributed file system (HDFS) while Spark is built on resilient distributed datasets (__RDDs__). Usually, we use Spark for data computation on top of Hadoop.  \n",
    "\n",
    "In this article, we will mainly talk about the data manipulation on __RDD__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28de7e8a-da48-4e22-a50f-df95e6ff586a",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "We will be using the movielens dataset (ml-100k) for demo.\n",
    "\n",
    "source: https://grouplens.org/datasets/movielens/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25200df7-6bbf-4bef-bcd2-2307469f861b",
   "metadata": {},
   "source": [
    "### Reference\n",
    "\n",
    "More Information: https://www.datacamp.com/community/blog/pyspark-cheat-sheet-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5576edd7-f22f-4acd-92e8-e2b53255db86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark import SparkContext, StorageLevel, SparkConf\n",
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c7ff9e-ff95-438a-a931-70d1708dd871",
   "metadata": {},
   "source": [
    "### 1. Initializing Spark Environment\n",
    "\n",
    "Whenever we want to use the Spark Engine, we need to first initialize the Spark environment.  \n",
    "\n",
    "#### SparkContext\n",
    "\n",
    "The driver program will use the SparkContext to connect and communicate with the cluster and it coordinate the jobs with the resource management system.\n",
    "\n",
    "#### SparkSession\n",
    "\n",
    "Starting from Spark 2.0, SparkSession built the gateway to different data sources, such as SQL or Hive. In Spark 1.x, we need to define the SQLContext or HiveContext to communicate with the corresponding sources.\n",
    "\n",
    "_In short, we need to define the SparkSession if we want to use the Spark Dataframe or Dataset, otherwise SparkContext can execute the RDDs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4d2a507-dcda-4844-970f-a946fdf725f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "conf.set(\"spark.driver.memory\", \"4g\")\n",
    "conf.set(\"spark.executor.memory\", \"4g\")\n",
    "\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773d7644-c946-49a9-a42c-d59931ed42ab",
   "metadata": {},
   "source": [
    "### 2. Loading Data\n",
    "\n",
    "We are able to create the _rdd_ with iterable data or external data sources.\n",
    "\n",
    "We will first load the movielen datafile as text string, and do the manipulation later.\n",
    "\n",
    "_Note: SparkSession need to be defined before accessing the __rdd.toDF()__ function_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b92bc54f-8151-4813-a63d-2579db5e8eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/ml-100k/'\n",
    "data = [('apple', 10),('orange', 20),('peach', 100)]\n",
    "\n",
    "rdd = sc.parallelize(data)\n",
    "rdd_list = sc.parallelize(range(100))\n",
    "rdd_movie = sc.textFile(data_path+\"u.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "029cc56d-f2eb-45bd-9665-4880767a3168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|    _1| _2|\n",
      "+------+---+\n",
      "| apple| 10|\n",
      "|orange| 20|\n",
      "| peach|100|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to create the Dataframe, we can use rdd.toDF()\n",
    "\n",
    "dfFromRDD1 = rdd.toDF()\n",
    "dfFromRDD1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407b9633-8266-4763-a050-dfd88c876e2c",
   "metadata": {},
   "source": [
    "### 3. Data Retrieval\n",
    "\n",
    "By using the following commands, we are able to retrieve information of the data in the rdd.\n",
    "\n",
    "- getNumPartitions()\n",
    "- count(), countByKey(), countByValue()\n",
    "- collect(), collectAsMap(), take(_num_), top(_num_)\n",
    "- max(), min(), mean(), stdev(), variance(), histogram(_num of bin_), stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4a72fde-225a-49d4-b639-7c4d9f2ed092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['196\\t242\\t3\\t881250949', '186\\t302\\t3\\t891717742', '22\\t377\\t1\\t878887116']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_movie.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca50f402-27be-45fc-a59a-e632dd98d559",
   "metadata": {},
   "source": [
    "### 4. Data Processing\n",
    "\n",
    "Processing data in PySpark might reminds you about the __Pandas__ Dataframe. Similar to Pandas, PySpark also provide the functionality to _group, aggregate, sort, and reduce_. However, there are some functions which are similar but perform differently. \n",
    "\n",
    "#### map vs flatMap\n",
    "\n",
    "map and flatMap looks similar but they are different, and worth extra attention. \n",
    "\n",
    "The map function is able to maintain the list structure of the original data shape, and the flatMap will unpack the list structure and form a large list data structure. \n",
    "\n",
    "In the following cells, we will visualize the difference by separating the rdd_movie rdd with proper delimiter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53377092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['196', '242', '3', '881250949'],\n",
       " ['186', '302', '3', '891717742'],\n",
       " ['22', '377', '1', '878887116']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map function\n",
    "rdd_movie.map(lambda x: x.split(\"\\t\")).take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d481ef66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['196', '242', '3', '881250949', '186', '302', '3', '891717742']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatMap function\n",
    "rdd_movie.flatMap(lambda x: x.split(\"\\t\")).take(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243e7fae",
   "metadata": {},
   "source": [
    "#### groupBy vs groupByKey\n",
    "\n",
    "groupBy function will group the data based on the result from the function in the input, groupByKey will group the data based on the key of the original rdd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30142d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "['196', '242', '3', '881250949']\n",
      "['186', '302', '3', '891717742']\n",
      "['166', '346', '1', '886397596']\n",
      "['6', '86', '3', '883603013']\n"
     ]
    }
   ],
   "source": [
    "# we will show the groupBy function by separating every 10 userid\n",
    "# for example, [0,10,20..] userid will be in the same group\n",
    "\n",
    "groupby_sample = rdd_movie.map(lambda x: x.split(\"\\t\")) \\\n",
    "                            .groupBy(lambda x: int(x[0]) % 10).take(1)\n",
    "\n",
    "# we will only print 4 data in the list\n",
    "for x,y in groupby_sample:\n",
    "    print(x)\n",
    "    for val in list(y)[:4]:\n",
    "        print(val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b6d7438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "['22', '377', '1', '878887116']\n",
      "['166', '346', '1', '886397596']\n",
      "['181', '1081', '1', '878962623']\n",
      "['276', '796', '1', '874791932']\n"
     ]
    }
   ],
   "source": [
    "# to illustrate the groupbykey function, we will first transform the data into the \n",
    "# (K,V) pair. In this example, we define the rating to be the key and group by the rating\n",
    "\n",
    "groupbykey_sample = rdd_movie.map(lambda x: x.split(\"\\t\")) \\\n",
    "                            .keyBy(lambda x: x[2]) \\\n",
    "                            .groupByKey().take(1)\n",
    "\n",
    "# we will only print 4 data in the list\n",
    "for x,y in groupbykey_sample:\n",
    "    print(x)\n",
    "    for val in list(y)[:4]:\n",
    "        print(val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e230ddd3",
   "metadata": {},
   "source": [
    "#### reduce vs reduceByKey\n",
    "\n",
    "Before we dive into the code, we need to first realize that _reduce_ is an action and _reduceByKey_ is transformation. \n",
    "\n",
    "Definition on the original document: https://spark.apache.org/docs/2.2.0/rdd-programming-guide.html\n",
    "\n",
    "_reduce: Aggregate the elements of the dataset using a function func (which takes two arguments and returns one). The function should be commutative and associative so that it can be computed correctly in parallel._\n",
    "\n",
    "_reduceByKey: When called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function func, which must be of type (V,V) => V. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument._\n",
    "\n",
    "In this section, we will demonstrate some examples of the functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d342c382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('apple', 10, 'orange', 20, 'peach', 100)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will use the rdd data to demonstrate the reduce function. Similar to the \n",
    "# collect(), collectAsMap() functions, the reduce function will return the result\n",
    "\n",
    "rdd.reduce(lambda a,b: a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ad5369a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('196', (3, 1)), ('186', (3, 1)), ('22', (1, 1))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next, we can use reduceByKey to calculate the average rating by person\n",
    "# we transform the rdd to (userid, (sum of rating, count of rating)), then calculate the average rating \n",
    "\n",
    "user_rating_count_pair = rdd_movie.map(lambda x: x.split(\"\\t\")) \\\n",
    "                            .map(lambda x: tuple([x[0], (int(x[2]), 1)]))\n",
    "\n",
    "user_rating_count_pair.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb91c071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('22', (429, 128)), ('244', (869, 238)), ('115', (362, 92))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_sum_rating_count_pair = user_rating_count_pair.reduceByKey(lambda a,b: (a[0]+b[0], a[1]+b[1]))\n",
    "\n",
    "user_sum_rating_count_pair.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c1699a0-a1c0-4c0c-8407-1d771b7499fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('22', 3.3515625), ('244', 3.6512605042016806), ('115', 3.9347826086956523)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_avg_rating = user_sum_rating_count_pair.map(lambda x: tuple([x[0], x[1][0]/ x[1][1]]))\n",
    "user_avg_rating.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbbddad-1c4e-4494-897a-d78668483dd8",
   "metadata": {},
   "source": [
    "### 5. Sample usage on MovieLen dataset\n",
    "\n",
    "In this section, we will use the PySpark RDD to process the data of the movie, and retrieve the corresponding genre.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69b3673c-becb-47a4-b70d-d0f123691fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/ml-100k/'\n",
    "\n",
    "# first we load the data into rdd\n",
    "rdd_movie = sc.textFile(data_path+\"u.item\")\n",
    "rdd_genre = sc.textFile(data_path+\"u.genre\")\n",
    "\n",
    "rdd_movie = rdd_movie.map(lambda x: x.split(\"|\"))\n",
    "genre_map = rdd_genre.map(lambda x: x.split(\"|\")) \\\n",
    "                    .map(lambda x: tuple([int(x[1]),x[0]])).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f484ead0-e3e2-41cf-91a2-edc48ebc2756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'unknown',\n",
       " 1: 'Action',\n",
       " 2: 'Adventure',\n",
       " 3: 'Animation',\n",
       " 4: \"Children's\",\n",
       " 5: 'Comedy',\n",
       " 6: 'Crime',\n",
       " 7: 'Documentary',\n",
       " 8: 'Drama',\n",
       " 9: 'Fantasy',\n",
       " 10: 'Film-Noir',\n",
       " 11: 'Horror',\n",
       " 12: 'Musical',\n",
       " 13: 'Mystery',\n",
       " 14: 'Romance',\n",
       " 15: 'Sci-Fi',\n",
       " 16: 'Thriller',\n",
       " 17: 'War',\n",
       " 18: 'Western'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genre_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a967c034-dfd0-4ac7-b96a-10a15a9aca71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1',\n",
       "  'Toy Story (1995)',\n",
       "  '01-Jan-1995',\n",
       "  '',\n",
       "  'http://us.imdb.com/M/title-exact?Toy%20Story%20(1995)',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1',\n",
       "  '1',\n",
       "  '1',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From the data definition, the columns in u.items are\n",
    "# movie id | movie title | release date | video release date |\n",
    "# IMDb URL | unknown | Action | Adventure | Animation |\n",
    "# Children's | Comedy | Crime | Documentary | Drama | Fantasy |\n",
    "# Film-Noir | Horror | Musical | Mystery | Romance | Sci-Fi |\n",
    "# Thriller | War | Western |\n",
    "\n",
    "rdd_movie.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97ac73cb-cc1d-4688-b223-cc8a69c60902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we transform the data into [movieid, movie_name, list of genre]\n",
    "def process_movie_data(data, genre_map):\n",
    "    list_genre = []\n",
    "    for i, ix in enumerate(data):\n",
    "        # the genre columns start at index 5\n",
    "        genre_idx = i - 5\n",
    "        if genre_idx >= 0:\n",
    "            if ix == '1':\n",
    "                list_genre.append(genre_map[genre_idx])\n",
    "    \n",
    "    res = [data[0], data[1], list_genre]\n",
    "    return res\n",
    "\n",
    "rdd_movie_detail = rdd_movie.map(lambda x: process_movie_data(x, genre_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d42bcf0-61d9-4801-892f-12914721db89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', 'Toy Story (1995)', ['Animation', \"Children's\", 'Comedy']],\n",
       " ['2', 'GoldenEye (1995)', ['Action', 'Adventure', 'Thriller']]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_movie_detail.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6de29b-f982-4f6b-a872-ec09569f3c21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
