{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "infrared-abraham",
   "metadata": {},
   "source": [
    "## Model-based Recommendation System with Matrix Factorization - ALS Model\n",
    "\n",
    "In this example, we will build a recommendation system using the ALS model from __pyspark__.\n",
    "\n",
    "A sparse matrix __R__ can be built based on the data of user-item relation and their ratings.\n",
    "\n",
    "The goal of matrix factorization method is to separate the utility matrix into the __user latent matrix__ and the __product latent matrix__, such that\n",
    "\n",
    "$$R \\approx U * P$$\n",
    "\n",
    "There are a lot of methods to factorize the utility matrix, such as the Singular Value Decomposition, Probabilistic Latent Semantic Analysis. In Alternative Least Square (ALS), it is an iterative process to optimize the factorization model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "special-wages",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "In this example, we will be using the movielens dataset (ml-100k).\n",
    "\n",
    "source: https://grouplens.org/datasets/movielens/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "normal-illness",
   "metadata": {},
   "source": [
    "### Mathematics behind the ALS Model\n",
    "First we will define our object function using the loss function, and we can optimize the model by minimizing the loss function.\n",
    "\n",
    "Loss Function: $RMSE = \\sqrt{\\sum (real - prediction)^2/n}$, \n",
    "where real = $R$, and prediction = $U*P^T$\n",
    "\n",
    "Assume there are $m$ users and $n$ items, $R = m \\times n$, $U = m \\times k$ and  $P = n \\times k$ where $k$ is the __latent factors__.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "loss &= min (real-prediction)^2\\\\\n",
    "    &= min(R-U*P^T)^2 \\\\\n",
    "    &= min \\sum_{x,y}{(R_{x,y} - U_{x}*P_{y}^T)^2} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In order to avoid overfitting, we add $l2$ norm to our objective function, such that\n",
    "$$loss = min \\sum_{x,y}{(R_{x,y} - U_{x}*P_{y}^T)^2} + \\lambda(\\lVert U \\rVert^2+\\lVert P \\rVert^2)$$\n",
    "\n",
    "Next, we take the partial differentiation respect to U and P.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial loss}{\\partial U} &= 0 \\\\\n",
    "    &= \\frac{\\partial}{\\partial U} \\sum_{x,y}{(R_{x,y} - U_{x}*P_{y}^T)^2} + \\lambda(\\lVert U \\rVert^2+\\lVert P \\rVert^2) = 0\\\\\n",
    "    &= -2\\sum_{x,y}{(R_{x,y}-U_x*P_y^T)P_y+2\\lambda U_x = 0} \\\\\n",
    "    &=-(R_x-U_x^TP^T)P + \\lambda U_x^T=0 \\\\\n",
    "    &= U_x^T = R_xP(P^TP+\\lambda I)^{-1}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial loss}{\\partial P} &= 0 \\\\\n",
    "    &= \\frac{\\partial}{\\partial P} \\sum_{x,y}{(R_{x,y} - U_{x}*P_{y}^T)^2} + \\lambda(\\lVert U \\rVert^2+\\lVert P \\rVert^2) = 0\\\\\n",
    "    &= P_y^T = R_yU(U^TU+\\lambda I)^{-1}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Therefore, we have both equations of $U_x$ and $P_y$. By fixing one, we can optimize the other one. Iteratively alternate the latent matrix $U_x$ and $P_y$, we are able to optimize the utility matrix factorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "junior-russell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "import pandas as pd\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.executor.memory\",\"6g\")\n",
    "conf.set(\"spark.driver.memory\", \"6g\")\n",
    "conf.set(\"spark.driver.cores\", \"8\")\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "spark = SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adjusted-lender",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(userid=196.0, itemid=242.0, rating=3.0),\n",
       " Row(userid=186.0, itemid=302.0, rating=3.0)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data as dataframe\n",
    "movielens = sc.textFile('../data/ml-100k/u.data').map(lambda x: tuple(x.split('\\t'))) \\\n",
    "                .map(lambda x: tuple([float(x[0]), float(x[1]), float(x[2])]))\n",
    "\n",
    "data = movielens.toDF(['userid','itemid','rating'])\n",
    "data.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "automotive-bridges",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "verified-writing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we create the train and test dataset\n",
    "train, test = data.randomSplit([0.7,0.3],7856)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecological-witch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[userid: double, itemid: double, rating: double]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.cache()\n",
    "test.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governmental-capitol",
   "metadata": {},
   "source": [
    "In the spark ALS model, we are able to define __rank__, __maxIter__, __regParam__, and more can be found on https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.recommendation.ALS.html#pyspark.ml.recommendation.ALS\n",
    "\n",
    "ALS model usually converge fast, so we put maxIter = 10, and __rank__ is equal to the number of __latent factors__.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "administrative-measurement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the cross validator to tune the hyperparameters\n",
    "als = ALS(\n",
    "         userCol=\"userid\", \n",
    "         itemCol=\"itemid\",\n",
    "         ratingCol=\"rating\", \n",
    "         coldStartStrategy=\"drop\"\n",
    ")\n",
    "\n",
    "param_grid = ParamGridBuilder() \\\n",
    "            .addGrid(als.rank, [10, 100]) \\\n",
    "            .addGrid(als.regParam, [.1]) \\\n",
    "            .addGrid(als.maxIter, [10]) \\\n",
    "            .build()\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "           metricName=\"rmse\", \n",
    "           labelCol=\"rating\", \n",
    "           predictionCol=\"prediction\")\n",
    "\n",
    "cv = CrossValidator(estimator=als, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=3, parallelism = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adjustable-emerald",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cv.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dimensional-scott",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank = 100\n",
      "MaxIter = 10\n",
      "RegParam = 0.1\n"
     ]
    }
   ],
   "source": [
    "best_model = model.bestModel\n",
    "\n",
    "print(f\"Rank = {best_model._java_obj.parent().getRank()}\")\n",
    "# Print \"MaxIter\"\n",
    "print(f\"MaxIter = {best_model._java_obj.parent().getMaxIter()}\")\n",
    "# Print \"RegParam\"\n",
    "print(f\"RegParam = {best_model._java_obj.parent().getRegParam()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "smaller-karen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 0.9293164895244701\n"
     ]
    }
   ],
   "source": [
    "prediction = best_model.transform(test)\n",
    "rmse = evaluator.evaluate(prediction)\n",
    "print(f'RMSE = {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dated-headset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|            features|\n",
      "+---+--------------------+\n",
      "| 10|[0.3314802, -0.07...|\n",
      "| 20|[0.3870823, 0.024...|\n",
      "| 30|[0.3141748, -0.09...|\n",
      "| 40|[0.26709092, -0.2...|\n",
      "| 50|[0.35146096, -0.0...|\n",
      "| 60|[0.36123818, -0.1...|\n",
      "| 70|[0.44221446, -0.0...|\n",
      "| 80|[0.4122668, -0.07...|\n",
      "| 90|[0.070995346, -0....|\n",
      "|100|[0.45245463, 0.05...|\n",
      "|110|[0.2630007, -0.06...|\n",
      "|120|[0.1782444, -0.15...|\n",
      "|130|[0.38577065, -0.1...|\n",
      "|140|[0.41791943, 0.01...|\n",
      "|150|[0.36449644, -0.2...|\n",
      "|160|[0.34152013, -0.3...|\n",
      "|170|[0.33740997, -0.2...|\n",
      "|180|[0.056818523, -0....|\n",
      "|190|[0.36812487, -0.0...|\n",
      "|200|[0.36409047, 0.00...|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+--------------------+\n",
      "| id|            features|\n",
      "+---+--------------------+\n",
      "| 10|[0.24241637, -0.2...|\n",
      "| 20|[0.21774949, -0.0...|\n",
      "| 30|[0.2805123, -0.04...|\n",
      "| 40|[0.08548905, -0.0...|\n",
      "| 50|[0.47504237, -0.2...|\n",
      "| 60|[0.046063706, -0....|\n",
      "| 70|[0.21902403, -0.1...|\n",
      "| 80|[0.33022547, -0.1...|\n",
      "| 90|[0.42008877, -0.0...|\n",
      "|100|[0.28412965, -0.1...|\n",
      "|110|[0.37525526, -0.1...|\n",
      "|120|[0.041536205, 0.1...|\n",
      "|130|[0.19771461, 0.00...|\n",
      "|140|[0.19416378, -0.1...|\n",
      "|150|[0.2960126, -0.23...|\n",
      "|160|[0.32059, -0.4187...|\n",
      "|170|[0.31297144, -0.3...|\n",
      "|180|[0.41809162, -0.1...|\n",
      "|190|[0.31688893, -0.0...|\n",
      "|200|[0.2200648, 0.026...|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we can get the user latent factors and item latent factors from the model\n",
    "best_model.userFactors.show()\n",
    "best_model.itemFactors.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consistent-bennett",
   "metadata": {},
   "source": [
    "#### Recommendations based on the model\n",
    "\n",
    "we can find the item recommendations to specific users or users who might be interested in specific item using the following methods.\n",
    "\n",
    "__recommendForAllUsers__, __recommendForAllItems__\n",
    "\n",
    "\n",
    "__recommendForUserSubset__, __recommendForItemSubset__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "delayed-patch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|userid|     recommendations|\n",
      "+------+--------------------+\n",
      "|   471|[[932, 4.677558],...|\n",
      "|   463|[[887, 4.2685623]...|\n",
      "|   833|[[1597, 4.7141786...|\n",
      "|   496|[[1240, 4.508081]...|\n",
      "|   148|[[1449, 4.908884]...|\n",
      "|   540|[[1449, 4.8743043...|\n",
      "|   392|[[187, 4.9549775]...|\n",
      "|   243|[[1449, 4.533235]...|\n",
      "|   623|[[174, 4.5522203]...|\n",
      "|   737|[[127, 4.716929],...|\n",
      "|   897|[[1368, 4.895013]...|\n",
      "|   858|[[9, 4.339468], [...|\n",
      "|    31|[[705, 4.6753798]...|\n",
      "|   516|[[1449, 4.6160107...|\n",
      "|   580|[[1368, 4.4243093...|\n",
      "|   251|[[1368, 4.7131], ...|\n",
      "|   451|[[333, 4.1837206]...|\n",
      "|    85|[[1449, 4.3844137...|\n",
      "|   137|[[50, 5.1624665],...|\n",
      "|   808|[[1368, 5.445765]...|\n",
      "+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# recommendation to all users\n",
    "best_model.recommendForAllUsers(3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "altered-courtesy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
